<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ContextRAG | Sean Brar </title> <meta name="author" content="Sean Brar"> <meta name="description" content="A scalable vector database system for semantic search with context-aware processing"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, dreaming-ideas, technology, artificial-intelligence, ai, blog, brar"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/faviconbasicoutline.ico?b92b8ecdcd4798e9ff9a1dc01aa94e26"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://seanbrar.com/projects/contextrag/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <div class="navbar-brand-inner-wrapper"> <img src="/assets/img/starlogo.png" alt="Logo" class="navbar-logo"> <span class="font-weight-bold">Sean</span> Brar </div> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">research </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">library </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">ContextRAG</h1> <p class="post-description">A scalable vector database system for semantic search with context-aware processing</p> </header> <article class="projects project-content"> <h2 id="introduction">Introduction</h2> <p>ContextRAG is a context-aware retrieval-augmented generation system designed to address fundamental limitations in LLM applications when processing documents of varying lengths and complexities. By implementing intelligent document classification and adaptive processing strategies, the system achieves improved semantic coherence and retrieval precision compared to traditional RAG approaches.</p> <h2 id="research-motivation">Research Motivation</h2> <p>Large language models have revolutionized many NLP tasks but face significant constraints when processing lengthy documents due to context window limitations. These constraints create several challenges for real-world LLM applications:</p> <ul> <li>Loss of semantic coherence across document chunks when traditional uniform chunking strategies are applied</li> <li>Inefficient token utilization in embedding models, leading to higher computational costs</li> <li>Poor retrieval performance for complex hierarchical documents where context and document structure are critical</li> </ul> <p>ContextRAG addresses these limitations through a novel adaptive processing approach based on document characteristics, enabling more nuanced and contextually relevant information retrieval while optimizing computational resources.</p> <h2 id="system-architecture">System Architecture</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/diagrams/contextrag-architecture.svg" sizes="95vw"></source> <img src="/assets/img/diagrams/contextrag-architecture.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ContextRAG Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ContextRAG's modular architecture enables efficient processing of documents regardless of size through specialized pipelines. </div> <p>The system implements a three-stage pipeline:</p> <ol> <li> <strong>Document Ingestion &amp; Classification</strong>: Documents are processed and classified based on length and complexity metrics</li> <li> <strong>Semantic Analysis &amp; Embedding Generation</strong>: Context-aware embedding strategies are applied based on document classification</li> <li> <strong>Vector Storage &amp; Retrieval</strong>: Optimized vector representations are stored and retrieved using similarity-based search</li> </ol> <p>This architecture provides a flexible framework for handling diverse document collections while maintaining retrieval efficiency and semantic precision.</p> <h2 id="technical-implementation">Technical Implementation</h2> <p>The technical implementation of ContextRAG focuses on three key innovations: length-based document classification, optimized semantic similarity computation, and context-aware processing strategies. Each component addresses specific challenges in building effective RAG systems.</p> <h3 id="length-based-document-classification">Length-Based Document Classification</h3> <p>A fundamental innovation in ContextRAG is its adaptive approach to documents based on token length. Rather than applying a one-size-fits-all processing strategy, the system analyzes document length and complexity to determine the optimal processing approach.</p> <p>This classification is implemented in <code class="language-plaintext highlighter-rouge">src/data_processing/html_to_markdown.py</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_get_target_folder</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">content</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Determine the target folder based on the content length.
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">content</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="mi">3500</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">folder_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">short</span><span class="sh">"</span>
    <span class="k">elif</span> <span class="mi">3500</span> <span class="o">&lt;</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">content</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">15000</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">folder_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">medium</span><span class="sh">"</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">folder_path</span> <span class="o">/</span> <span class="sh">"</span><span class="s">long</span><span class="sh">"</span>
</code></pre></div></div> <p>This classification system enables:</p> <ol> <li> <strong>Processing Optimization</strong> - Different strategies for different document lengths</li> <li> <strong>Model Selection</strong> - Appropriate model choice based on token constraints</li> <li> <strong>Storage Organization</strong> - Efficient document categorization for retrieval</li> </ol> <p>The token counting is implemented using the <code class="language-plaintext highlighter-rouge">tiktoken</code> library for accurate token estimation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Count tokens in a text string using tiktoken.</span><span class="sh">"""</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="p">.</span><span class="nf">get_encoding</span><span class="p">(</span><span class="sh">"</span><span class="s">cl100k_base</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">encoding</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
</code></pre></div></div> <p>By accurately estimating token counts, the system can make informed decisions about document processing strategies, ensuring optimal use of computational resources while maintaining semantic coherence.</p> <h3 id="semantic-similarity-computation">Semantic Similarity Computation</h3> <p>ContextRAG implements sophisticated similarity computation using OpenAI embeddings and cosine similarity. The implementation balances embedding quality with computational efficiency through several optimization techniques.</p> <p>The core similarity computation is implemented in <code class="language-plaintext highlighter-rouge">src/markdown_grouping/file_grouping.py</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_similarity</span><span class="p">(</span><span class="n">files_dict</span><span class="p">,</span> <span class="n">checksums</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Compute similarity between files using OpenAI embeddings.</span><span class="sh">"""</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">filename</span><span class="p">,</span> <span class="n">content</span> <span class="ow">in</span> <span class="n">files_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">checksum</span> <span class="o">=</span> <span class="n">checksums</span><span class="p">[</span><span class="n">filename</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">checksum</span> <span class="ow">in</span> <span class="n">cache</span><span class="p">:</span>
            <span class="n">embeddings</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="n">checksum</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">processed_text</span> <span class="o">=</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
            <span class="n">token_count</span> <span class="o">=</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="n">processed_text</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">token_count</span> <span class="o">&lt;=</span> <span class="mi">8000</span><span class="p">:</span>
                <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
                    <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">text-embedding-3-large</span><span class="sh">"</span><span class="p">,</span>
                    <span class="nb">input</span><span class="o">=</span><span class="n">processed_text</span><span class="p">,</span>
                    <span class="n">encoding_format</span><span class="o">=</span><span class="sh">"</span><span class="s">float</span><span class="sh">"</span><span class="p">,</span>
                    <span class="n">dimensions</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">embedding</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">embedding</span>
                <span class="n">embeddings</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
                <span class="n">cache</span><span class="p">[</span><span class="n">checksum</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>

    <span class="c1"># Normalize the embeddings before computing the cosine similarity
</span>    <span class="n">normalized_embeddings</span> <span class="o">=</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
    <span class="n">similarity_matrix</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">normalized_embeddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">similarity_matrix</span>
</code></pre></div></div> <p>This implementation includes several key optimizations:</p> <ol> <li> <strong>Embedding Caching</strong> - Reuses embeddings for identical content, reducing API calls and computational overhead</li> <li> <strong>Token Limit Handling</strong> - Ensures content fits within model constraints, preventing truncation issues</li> <li> <strong>Normalization</strong> - Improves consistency of similarity scores across documents of varying lengths</li> <li> <strong>High-Dimensional Embeddings</strong> - Uses 3072-dimension embeddings for greater precision in semantic representation</li> </ol> <p>These optimizations enable efficient similarity computation even for large document collections, making the system viable for real-world applications with diverse content types.</p> <h3 id="context-aware-processing-strategy">Context-Aware Processing Strategy</h3> <p>A core innovation in ContextRAG is its differentiated processing strategies based on document characteristics. The system employs distinct approaches for documents of varying lengths, optimizing both computational efficiency and semantic coherence.</p> <table> <thead> <tr> <th>Document Type</th> <th>Token Range</th> <th>Processing Approach</th> <th>Implementation Details</th> </tr> </thead> <tbody> <tr> <td><strong>Short</strong></td> <td>≤3,500</td> <td>Full-context embedding</td> <td>Direct embedding with text-embedding-3-large (3072 dims)</td> </tr> <tr> <td><strong>Medium</strong></td> <td>3,500-15,000</td> <td>Model adaptation</td> <td>Uses GPT-3.5-Turbo-16K for processing</td> </tr> <tr> <td><strong>Long</strong></td> <td>&gt;15,000</td> <td>Specialized handling</td> <td>Custom chunking and hierarchical embedding</td> </tr> </tbody> </table> <p>This approach is reflected in the model selection logic in <code class="language-plaintext highlighter-rouge">src/markdown_grouping/category_assignment.py</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Determine the model based on the token count
</span><span class="k">if</span> <span class="n">token_count</span> <span class="o">&lt;=</span> <span class="mi">3500</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ChatModels</span><span class="p">.</span><span class="n">GPT_3_5_TURBO_1106</span>
<span class="k">elif</span> <span class="mi">3500</span> <span class="o">&lt;</span> <span class="n">token_count</span> <span class="o">&lt;</span> <span class="mi">15000</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ChatModels</span><span class="p">.</span><span class="n">GPT_3_5_TURBO_16K</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Skipped </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s"> due to excessive token count (</span><span class="si">{</span><span class="n">token_count</span><span class="si">}</span><span class="s"> tokens).</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">continue</span>
</code></pre></div></div> <p>By selecting models and processing strategies based on document characteristics, ContextRAG achieves better performance and cost efficiency compared to uniform processing approaches.</p> <h3 id="vector-database-integration">Vector Database Integration</h3> <p>Efficient storage and retrieval of vector embeddings is essential for system performance. ContextRAG integrates ChromaDB, a vector database optimized for similarity search, to enable fast and accurate document retrieval.</p> <p>The vector database integration is implemented in <code class="language-plaintext highlighter-rouge">src/vector_db/main.py</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">query</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">query_texts</span><span class="p">,</span> <span class="n">n_results</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Query the collection for similar documents.</span><span class="sh">"""</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">collection</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span>
        <span class="n">query_texts</span><span class="o">=</span><span class="n">query_texts</span><span class="p">,</span>
        <span class="n">n_results</span><span class="o">=</span><span class="n">n_results</span><span class="p">,</span>
        <span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">documents</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">distances</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div> <p>This implementation enables:</p> <ol> <li> <strong>Scalable vector storage</strong> - Efficient handling of large document collections</li> <li> <strong>Fast similarity search</strong> - Optimized retrieval of relevant documents</li> <li> <strong>Distance-based ranking</strong> - Documents ranked by semantic similarity</li> </ol> <p>The vector database provides a scalable foundation for the system, enabling efficient retrieval even as document collections grow in size and complexity.</p> <h2 id="experimental-analysis">Experimental Analysis</h2> <p>While formal benchmarks are still in development, initial testing of ContextRAG demonstrates several qualitative improvements over traditional RAG approaches:</p> <ol> <li> <strong>Retrieval Precision</strong> - More accurate document retrieval by preserving semantic context</li> <li> <strong>Processing Efficiency</strong> - Optimized token usage and model selection</li> <li> <strong>Scalability</strong> - Effective handling of documents regardless of length or complexity</li> </ol> <p>These improvements are particularly noticeable when processing complex technical documentation, research papers, and hierarchical content where context preservation is critical for accurate retrieval.</p> <p>The system shows particular promise for applications requiring high retrieval precision, such as:</p> <ul> <li>Technical documentation search</li> <li>Research literature review</li> <li>Legal document analysis</li> <li>Knowledge base management</li> </ul> <h2 id="limitations-and-evolution-of-context-windows">Limitations and Evolution of Context Windows</h2> <p>While ContextRAG provides effective solutions for context management, several limitations and evolving factors should be acknowledged:</p> <h3 id="model-selection-trade-offs">Model Selection Trade-offs</h3> <p>The system currently uses GPT-3.5 Turbo models (4K and 16K context variants) due to favorable cost-performance trade-offs during initial development. This represents a deliberate engineering decision balancing:</p> <ul> <li> <strong>Latency Requirements</strong> - Larger models typically have higher inference times</li> <li> <strong>Cost Considerations</strong> - Significant cost differences between model tiers</li> <li> <strong>Retrieval Quality</strong> - Diminishing returns beyond certain context sizes</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Current model selection approach
</span><span class="k">if</span> <span class="n">token_count</span> <span class="o">&lt;=</span> <span class="mi">3500</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ChatModels</span><span class="p">.</span><span class="n">GPT_3_5_TURBO_1106</span>  <span class="c1"># Lower cost, faster inference
</span><span class="k">elif</span> <span class="mi">3500</span> <span class="o">&lt;</span> <span class="n">token_count</span> <span class="o">&lt;</span> <span class="mi">15000</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">ChatModels</span><span class="p">.</span><span class="n">GPT_3_5_TURBO_16K</span>   <span class="c1"># Higher cost, manageable latency
</span></code></pre></div></div> <h3 id="context-window-evolution">Context Window Evolution</h3> <p>Recent advances in model architecture have dramatically increased context windows:</p> <table> <thead> <tr> <th>Time Period</th> <th>Leading Models</th> <th>Typical Context Windows</th> </tr> </thead> <tbody> <tr> <td>2022–2023</td> <td>GPT-3.5, Claude 1, LLaMA 1</td> <td>2K–16K tokens</td> </tr> <tr> <td>2023–2024</td> <td>GPT-4, Claude 2, LLaMA 2</td> <td>4K–32K tokens</td> </tr> <tr> <td>2024–2025</td> <td>GPT-4o, Gemini 1.5 Pro, LLaMA 3, DeepSeek-V3</td> <td>128K–2M tokens</td> </tr> </tbody> </table> <p>Despite these advances, context limitations remain relevant for several reasons:</p> <ol> <li> <strong>Scale Gap</strong> - Many real-world document collections exceed even million-token context windows</li> <li> <strong>Attention Mechanism Limitations</strong> - Performance degradation with extremely long contexts</li> <li> <strong>Inference Cost</strong> - Quadratic scaling of computational costs with context length</li> <li> <strong>Retrieval Precision</strong> - Diminishing quality of retrievals in extremely large contexts</li> </ol> <p>These limitations highlight the continued relevance of intelligent context management systems even as context windows expand.</p> <h3 id="future-research-directions">Future Research Directions</h3> <p>Current work is focused on:</p> <ol> <li> <strong>Adaptive Model Selection</strong> - Dynamic selection of models based on document characteristics and query requirements</li> <li> <strong>Hierarchical Embedding Strategies</strong> - Developing multi-level embeddings for very long documents</li> <li> <strong>Cross-Document Context Preservation</strong> - Maintaining relationships between related documents</li> <li> <strong>Query Optimization</strong> - Intelligent query reformulation based on document characteristics</li> <li> <strong>Evaluation Framework</strong> - Comprehensive benchmarking against traditional RAG systems</li> </ol> <h2 id="conclusion">Conclusion</h2> <p>ContextRAG demonstrates that context-aware processing strategies can significantly improve retrieval performance in RAG systems. By adapting processing approaches based on document characteristics, the system achieves better semantic coherence, more efficient resource utilization, and improved retrieval precision compared to uniform processing approaches.</p> <p>As language models continue to evolve, the principles of context-aware document processing will remain relevant for optimizing retrieval performance and computational efficiency in real-world applications.</p> <p>The project is open source and available on <a href="https://github.com/seanbrar/ContextRAG" rel="external nofollow noopener" target="_blank">GitHub</a>. Contributions and feedback from the community is welcome as I continue to develop and refine the system.</p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Sean Brar. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>